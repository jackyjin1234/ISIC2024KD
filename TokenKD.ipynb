{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "703d7457",
   "metadata": {},
   "source": [
    "# Token-Based Knowledge Distillation for ISIC 2024 Skin Cancer Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd72613",
   "metadata": {},
   "source": [
    "This notebook is an implementation of token-based knowledge distillation for the ISIC 2024 Skin Cancer Detection with 3D-TBP challenge. \n",
    "\n",
    "The approach leverages a pre-trained Swin Transformer V2 Small model as the teacher and a smaller Vision Transformer (ViT) as the student model to create an efficient yet accurate skin cancer detection system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e71baa5",
   "metadata": {},
   "source": [
    "## Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6518099a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torch.amp import GradScaler, autocast\n",
    "from io import BytesIO\n",
    "import h5py\n",
    "import timm\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8937cd5f",
   "metadata": {},
   "source": [
    "## Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e88dad16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISICDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels=None, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx]\n",
    "            return image, label\n",
    "        else:\n",
    "            return image\n",
    "\n",
    "# Define data transformations\n",
    "def get_transforms(mode='train'):\n",
    "    if mode == 'train':\n",
    "        return transforms.Compose([\n",
    "            transforms.RandomResizedCrop(256),  # Changed from 224\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomVerticalFlip(),\n",
    "            transforms.RandomRotation(15),\n",
    "            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD)\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize(256),        # Changed from 256->224\n",
    "            transforms.CenterCrop(256),    # Changed from 224\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD)\n",
    "        ])\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "def prepare_data(data_dir=\"Data\", max_label0_train=40000, val_label0_size=10000, val_label1_ratio=0.2, batch_size=32):\n",
    "    # Load image paths\n",
    "    benign_dir = os.path.join(data_dir, \"train-image-0_hairRemove\")\n",
    "    malignant_dir = os.path.join(data_dir, \"train-image-1_hairRemove\")\n",
    "    \n",
    "    benign_images = [os.path.join(benign_dir, img) for img in os.listdir(benign_dir) if img.endswith('.jpg')]\n",
    "    malignant_images = [os.path.join(malignant_dir, img) for img in os.listdir(malignant_dir) if img.endswith('.jpg')]\n",
    "\n",
    "    # Process label0 (benign)\n",
    "    np.random.seed(42)\n",
    "    benign_train = np.random.choice(benign_images, size=max_label0_train, replace=False)\n",
    "    remaining_benign = list(set(benign_images) - set(benign_train))\n",
    "    benign_val = np.random.choice(remaining_benign, size=val_label0_size, replace=False)\n",
    "\n",
    "    # Process label1 (malignant) - keep all in training and split a small portion for validation\n",
    "    malignant_train, malignant_val = train_test_split(\n",
    "        malignant_images, \n",
    "        test_size=val_label1_ratio, \n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Create final datasets\n",
    "    train_imgs = list(benign_train) + list(malignant_train)\n",
    "    train_labels = [0]*len(benign_train) + [1]*len(malignant_train)\n",
    "    \n",
    "    val_imgs = list(benign_val) + list(malignant_val)\n",
    "    val_labels = [0]*len(benign_val) + [1]*len(malignant_val)\n",
    "\n",
    "    # Shuffle training data\n",
    "    train_combined = list(zip(train_imgs, train_labels))\n",
    "    np.random.shuffle(train_combined)\n",
    "    train_imgs, train_labels = zip(*train_combined)\n",
    "\n",
    "    print(f\"Training set: {len(train_imgs)} images (0: {len(benign_train)}, 1: {len(malignant_train)})\")\n",
    "    print(f\"Validation set: {len(val_imgs)} images (0: {len(benign_val)}, 1: {len(malignant_val)})\")\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = ISICDataset(train_imgs, train_labels, transform=get_transforms('train'))\n",
    "    val_dataset = ISICDataset(val_imgs, val_labels, transform=get_transforms('val'))\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2830e3",
   "metadata": {},
   "source": [
    "## Teacher Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "81194cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_teacher_model():\n",
    "    model = timm.create_model('swinv2_small_window8_256', \n",
    "                            pretrained=False, \n",
    "                            num_classes=2)\n",
    "    model.load_state_dict(torch.load('clean_swin_weights.pth'))\n",
    "    return model.to(device).eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af28bfb2",
   "metadata": {},
   "source": [
    "## Student Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8736ee71",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistillableViT(nn.Module):\n",
    "    def __init__(self, image_size=256, patch_size=16, num_classes=2, dim=384,  # Changed image_size\n",
    "                 depth=12, heads=6, mlp_dim=1536, dropout=0.1, emb_dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vit = timm.create_model(\n",
    "            'vit_small_patch16_224', \n",
    "            pretrained=True,\n",
    "            num_classes=num_classes,\n",
    "            img_size=image_size,  # Now 256\n",
    "            patch_size=patch_size\n",
    "        )\n",
    "        # torch.save(self.vit.state_dict(), 'vit_weights.pth')\n",
    "        \n",
    "        # Get the feature dimension of the ViT\n",
    "        self.dim = dim\n",
    "        \n",
    "        # Add a distillation token alongside the class token\n",
    "        self.distillation_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        \n",
    "        # Adjust the position embedding to account for the extra token\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "        pos_embedding = self.vit.pos_embed\n",
    "        \n",
    "        # Create new position embeddings to include the distillation token\n",
    "        new_pos_embed = nn.Parameter(torch.zeros(1, num_patches + 2, dim))\n",
    "        # Copy existing embeddings\n",
    "        new_pos_embed.data[:, 0:1, :] = pos_embedding.data[:, 0:1, :]  # class token\n",
    "        new_pos_embed.data[:, 2:, :] = pos_embedding.data[:, 1:, :]    # patch tokens\n",
    "        \n",
    "        self.vit.pos_embed = new_pos_embed\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "\n",
    "        # Get the class token and patch embeddings from ViT\n",
    "        x = self.vit.patch_embed(x)\n",
    "        cls_token = self.vit.cls_token.expand(b, -1, -1)\n",
    "        dist_token = self.distillation_token.expand(b, -1, -1)\n",
    "\n",
    "        # Concatenate tokens\n",
    "        x = torch.cat((cls_token, dist_token, x), dim=1)\n",
    "        \n",
    "        # Add position embeddings\n",
    "        x = x + self.vit.pos_embed\n",
    "        x = self.vit.pos_drop(x)\n",
    "        \n",
    "        # Apply transformer blocks\n",
    "        for blk in self.vit.blocks:\n",
    "            x = blk(x)\n",
    "        \n",
    "        x = self.vit.norm(x)\n",
    "        \n",
    "        # Get only the class token output for inference\n",
    "        cls_token_out = x[:, 0]\n",
    "        \n",
    "        # Return logits\n",
    "        return self.vit.head(cls_token_out), cls_token_out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20215d6",
   "metadata": {},
   "source": [
    "## Knowledge Distillation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "dd0b8cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeDistillationLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, temperature=3.0, feature_adapter=None, class_weights=None):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.temperature = temperature\n",
    "\n",
    "        # Add class weights to cross entropy loss\n",
    "        if class_weights is not None:\n",
    "            self.ce_loss = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        else:\n",
    "            self.ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "        self.kl_loss = nn.KLDivLoss(reduction='batchmean')\n",
    "        self.feature_adapter = feature_adapter\n",
    "        \n",
    "    def forward(self, student_logits, student_features, \n",
    "               teacher_logits, teacher_features, labels):\n",
    "        # Project teacher features if needed\n",
    "        if self.feature_adapter:\n",
    "            teacher_features = self.feature_adapter(teacher_features)\n",
    "            \n",
    "        # Rest of the loss calculation remains the same\n",
    "        student_cls_loss = self.ce_loss(student_logits, labels)\n",
    "        \n",
    "        soft_targets = nn.functional.softmax(teacher_logits / self.temperature, dim=1)\n",
    "        soft_predictions = nn.functional.log_softmax(student_logits / self.temperature, dim=1)\n",
    "        soft_loss = self.kl_loss(soft_predictions, soft_targets) * (self.temperature ** 2)\n",
    "        \n",
    "        feature_loss = nn.functional.mse_loss(student_features, teacher_features)\n",
    "        \n",
    "        loss = (1 - self.alpha) * student_cls_loss + \\\n",
    "               self.alpha * (0.7 * soft_loss + 0.3 * feature_loss)\n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33af7c0b",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5abd604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pAUC(val_targets, val_preds, min_tpr=0.80):\n",
    "    \"\"\"Calculate partial AUC above specified TPR threshold.\"\"\"\n",
    "    v_gt = 1 - np.array(val_targets)  # Invert labels (malignant ↔ benign)\n",
    "    v_pred = -np.array(val_preds)     # Negate predictions for ranking reversal\n",
    "    \n",
    "    max_fpr = 1 - min_tpr  # Convert TPR threshold to FPR limit\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(v_gt, v_pred)\n",
    "    stop_idx = np.searchsorted(fpr, max_fpr, side='right')\n",
    "    \n",
    "    if stop_idx == 0:\n",
    "        return 0.0\n",
    "    elif stop_idx == len(fpr):\n",
    "        return auc(fpr, tpr)\n",
    "    else:\n",
    "        # Linear interpolation at boundary\n",
    "        x_interp = [fpr[stop_idx-1], fpr[stop_idx]]\n",
    "        y_interp = [tpr[stop_idx-1], tpr[stop_idx]]\n",
    "        interp_tpr = np.interp(max_fpr, x_interp, y_interp)\n",
    "        \n",
    "        partial_fpr = np.append(fpr[:stop_idx], max_fpr)\n",
    "        partial_tpr = np.append(tpr[:stop_idx], interp_tpr)\n",
    "        return auc(partial_fpr, partial_tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1e121022",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_distillation(teacher_model, student_model, train_loader, val_loader, \n",
    "                           num_epochs=10, lr=1e-4, weight_decay=1e-5,\n",
    "                           alpha=0.5, temperature=3.0, feature_adapter=None,\n",
    "                           patience=5, delta=0.001):\n",
    "    # Initialize GradScaler for mixed precision\n",
    "    scaler = torch.amp.GradScaler()\n",
    "\n",
    "    # Include feature_adapter in optimizer\n",
    "    optimizer_params = list(student_model.parameters())\n",
    "    if feature_adapter:\n",
    "        optimizer_params += list(feature_adapter.parameters())\n",
    "\n",
    "    # Define optimizer\n",
    "    optimizer = optim.AdamW(optimizer_params, lr=lr, weight_decay=weight_decay)\n",
    "    \n",
    "    # Define scheduler\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
    "    \n",
    "    # Define loss function\n",
    "    class_weights = torch.tensor([0.02, 1.0]).to(device)  # Adjust as needed\n",
    "    criterion = KnowledgeDistillationLoss(alpha=alpha, temperature=temperature,\n",
    "                                         feature_adapter=feature_adapter, class_weights=class_weights)\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_auc = 0.0\n",
    "    best_model_path = 'best_student_model.pth'\n",
    "    counter = 0  # Track epochs without improvement\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        student_model.train()\n",
    "        teacher_model.eval()\n",
    "        \n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Train]')\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in enumerate(progress_bar):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # # Get teacher predictions\n",
    "            # with torch.no_grad():\n",
    "            #     teacher_logits = teacher_model(inputs)\n",
    "            #     # Extract the last hidden state from the teacher\n",
    "            #     # Note: This is model-specific and may need to be adjusted\n",
    "            #     teacher_features = teacher_model.head.global_pool(\n",
    "            #         teacher_model.forward_features(inputs))\n",
    "            \n",
    "            # # Forward pass through student\n",
    "            # student_logits, student_features = student_model(inputs)\n",
    "            \n",
    "            # # Calculate loss\n",
    "            # loss = criterion(student_logits, student_features, \n",
    "            #                 teacher_logits, teacher_features, targets)\n",
    "            \n",
    "            # # Backward and optimize\n",
    "            # optimizer.zero_grad()\n",
    "            # loss.backward()\n",
    "            # optimizer.step()\n",
    "\n",
    "            # Mixed precision forward pass\n",
    "            with autocast(device_type=device.type):\n",
    "                # Get teacher predictions\n",
    "                with torch.no_grad():\n",
    "                    teacher_logits = teacher_model(inputs)\n",
    "                    teacher_features = teacher_model.head.global_pool(\n",
    "                        teacher_model.forward_features(inputs))\n",
    "                \n",
    "                # Student forward pass\n",
    "                student_logits, student_features = student_model(inputs)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(student_logits, student_features, \n",
    "                                teacher_logits, teacher_features, targets)\n",
    "\n",
    "            # Backward and optimize with scaler\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            # Calculate training metrics\n",
    "            _, predicted = student_logits.float().max(1)  # Cast to float32 for accuracy\n",
    "            train_total += targets.size(0)\n",
    "            train_correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # Calculate training accuracy\n",
    "            _, predicted = student_logits.max(1)\n",
    "            train_total += targets.size(0)\n",
    "            train_correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            train_loss += loss.item()\n",
    "            avg_loss = train_loss / (batch_idx + 1)\n",
    "            acc = 100. * train_correct / train_total\n",
    "            progress_bar.set_postfix(loss=avg_loss, acc=f'{acc:.2f}%')\n",
    "        \n",
    "        # Validation phase\n",
    "        student_model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        val_preds = []\n",
    "        val_targets = []\n",
    "        \n",
    "        progress_bar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{num_epochs} [Val]')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(progress_bar):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                \n",
    "                # Forward pass through student (in eval mode, returns only logits)\n",
    "                # outputs = student_model(inputs)\n",
    "                outputs = student_model(inputs).float()  # Cast to float32\n",
    "                \n",
    "                # Calculate validation accuracy\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += targets.size(0)\n",
    "                val_correct += predicted.eq(targets).sum().item()\n",
    "                \n",
    "                # Store predictions and targets for AUC calculation\n",
    "                val_preds.extend(torch.softmax(outputs, dim=1)[:, 1].cpu().numpy())\n",
    "                val_targets.extend(targets.cpu().numpy())\n",
    "                \n",
    "                # Update progress bar\n",
    "                acc = 100. * val_correct / val_total\n",
    "                progress_bar.set_postfix(acc=f'{acc:.2f}%')\n",
    "        \n",
    "        # Calculate validation AUC\n",
    "        val_auc = roc_auc_score(val_targets, val_preds)\n",
    "        val_prauc = average_precision_score(val_targets, val_preds)\n",
    "        val_pauc = calculate_pAUC(val_targets, val_preds, min_tpr=0.80)\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {avg_loss:.4f} | Train Acc: {100.*train_correct/train_total:.2f}%')\n",
    "        print(f'Val Acc: {100.*val_correct/val_total:.2f}% | Val AUC: {val_auc:.4f} | Val PR-AUC: {val_prauc:.4f} | Val pAUC: {val_pauc:.4f}')\n",
    "        \n",
    "        # Early stopping check\n",
    "        if val_pauc > best_val_auc + delta:  # Improvement threshold\n",
    "            best_val_auc = val_pauc\n",
    "            torch.save(student_model.state_dict(), best_model_path)\n",
    "            print(f'Saved best model with PR-AUC: {val_pauc:.4f}')\n",
    "            counter = 0  # Reset counter\n",
    "        else:\n",
    "            counter += 1\n",
    "            print(f'No improvement for {counter}/{patience} epochs')\n",
    "            if counter >= patience:\n",
    "                print(f'Early stopping at epoch {epoch+1}')\n",
    "                break  # Exit training loop\n",
    "\n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "    \n",
    "    # Load best model\n",
    "    student_model.load_state_dict(torch.load(best_model_path))\n",
    "    \n",
    "    return student_model, best_val_auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da92e0d",
   "metadata": {},
   "source": [
    "## Feature Adaptation Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4d7342",
   "metadata": {},
   "source": [
    "When distilling from a Swin Transformer to a ViT, we need to handle the difference in feature dimensions. Add a feature adaptation layer to align the teacher's features with the student's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3ea70647",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureAdapter(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.adapter = nn.Sequential(\n",
    "            nn.Linear(in_features, out_features),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(out_features, out_features)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.adapter(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b53609e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_pos_embed(pos_embed, new_num_tokens):\n",
    "    # pos_embed: [1, 258, 384] (student model WITH distillation token)\n",
    "    # new_num_tokens: 197 (inference model needs 197 tokens)\n",
    "    \n",
    "    # Separate tokens\n",
    "    cls_token = pos_embed[:, :1, :]    # [1, 1, 384] class token\n",
    "    dist_token = pos_embed[:, 1:2, :]  # [1, 1, 384] distillation token (to discard)\n",
    "    patch_embed = pos_embed[:, 2:, :]  # [1, 256, 384] original patches\n",
    "    \n",
    "    # Reshape patches to 2D grid (16x16 for 256x256 input)\n",
    "    old_size = int(patch_embed.shape[1] ** 0.5)  # 16\n",
    "    patch_embed = patch_embed.reshape(1, old_size, old_size, -1).permute(0, 3, 1, 2)\n",
    "    \n",
    "    # Calculate new grid size (14x14 for 224x224 input)\n",
    "    new_size = int((new_num_tokens - 1) ** 0.5)  # 14\n",
    "    \n",
    "    # Interpolate using bicubic\n",
    "    patch_embed = torch.nn.functional.interpolate(\n",
    "        patch_embed,\n",
    "        size=(new_size, new_size),\n",
    "        mode='bicubic',\n",
    "        align_corners=False\n",
    "    )\n",
    "    \n",
    "    # Re-flatten patches\n",
    "    patch_embed = patch_embed.permute(0, 2, 3, 1).reshape(1, -1, patch_embed.shape[1])\n",
    "    \n",
    "    # Combine with class token (discard distillation token)\n",
    "    new_pos_embed = torch.cat([cls_token, patch_embed], dim=1)  # [1, 197, 384]\n",
    "    \n",
    "    return new_pos_embed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b331ceb0",
   "metadata": {},
   "source": [
    "## Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f7301e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 30\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "ALPHA = 0.5  # Weight for distillation loss\n",
    "TEMPERATURE = 3.0  # Temperature for soft targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4207aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "train_loader, val_loader = prepare_data(batch_size=BATCH_SIZE)\n",
    "\n",
    "# Load teacher model\n",
    "teacher_model = load_teacher_model()\n",
    "\n",
    "# Create student model\n",
    "student_model = DistillableViT(image_size=256, patch_size=16, num_classes=2)\n",
    "student_model = student_model.to(device)\n",
    "\n",
    "# Create feature adapter if dimensions don't match\n",
    "feature_adapter = None\n",
    "teacher_feature_dim = teacher_model.num_features  # Swinv2 small: 768\n",
    "student_feature_dim = student_model.dim  # ViT small: 384\n",
    "\n",
    "if teacher_feature_dim != student_feature_dim:\n",
    "    feature_adapter = FeatureAdapter(teacher_feature_dim, student_feature_dim)\n",
    "    feature_adapter = feature_adapter.to(device)\n",
    "\n",
    "# Modify the training function to use feature_adapter if needed\n",
    "def modified_train_loop():\n",
    "    # Training logic\n",
    "    # This includes the feature adapter in the forward pass if needed\n",
    "    student_model.train()\n",
    "    teacher_model.eval()\n",
    "    \n",
    "    for inputs, targets in train_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Get teacher predictions\n",
    "        with torch.no_grad():\n",
    "            teacher_logits = teacher_model(inputs)\n",
    "            teacher_features = teacher_model.head.global_pool(\n",
    "                teacher_model.forward_features(inputs))\n",
    "            \n",
    "            # Adapt teacher features if needed\n",
    "            if feature_adapter:\n",
    "                teacher_features = feature_adapter(teacher_features)\n",
    "        \n",
    "        # Forward pass through student\n",
    "        student_logits, student_features = student_model(inputs)\n",
    "        \n",
    "        # Rest of the training loop\n",
    "        # ...\n",
    "\n",
    "# Pass feature_adapter to training function\n",
    "trained_student, best_auc = train_with_distillation(\n",
    "    teacher_model, student_model, train_loader, val_loader,\n",
    "    num_epochs=NUM_EPOCHS, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY,\n",
    "    alpha=ALPHA, temperature=TEMPERATURE, feature_adapter=feature_adapter,\n",
    "    patience=10, delta=0.001\n",
    ")   \n",
    "\n",
    "print(f\"Training completed. Best validation AUC: {best_auc:.4f}\")\n",
    "\n",
    "# Save the final model\n",
    "torch.save({\n",
    "    'model_state_dict': trained_student.state_dict(),\n",
    "    'best_auc': best_auc\n",
    "}, 'final_distilled_vit_small.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231d2132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and load trained student model\n",
    "trained_student = DistillableViT(image_size=256, patch_size=16, num_classes=2)\n",
    "trained_student.load_state_dict(torch.load('./best_student_model.pth'))\n",
    "trained_student.eval()\n",
    "\n",
    "# Get student state dict (excluding distillation token)\n",
    "student_state_dict = trained_student.vit.state_dict()\n",
    "\n",
    "# Create inference model\n",
    "inference_model = timm.create_model('vit_small_patch16_224', num_classes=2)\n",
    "inference_state_dict = inference_model.state_dict()\n",
    "\n",
    "# 1. Copy compatible weights\n",
    "for name, param in student_state_dict.items():\n",
    "    if name in inference_state_dict and name != 'pos_embed':\n",
    "        inference_state_dict[name] = param\n",
    "\n",
    "# 2. Handle positional embeddings\n",
    "# Student pos_embed shape: [1, 258, 384] (class + dist + 256 patches)\n",
    "# Need to convert to [1, 197, 384] (class + 196 patches)\n",
    "new_pos_embed = interpolate_pos_embed(\n",
    "    student_state_dict['pos_embed'],\n",
    "    inference_state_dict['pos_embed'].shape[1]\n",
    ")\n",
    "inference_state_dict['pos_embed'] = new_pos_embed\n",
    "\n",
    "# 3. Handle cls_token and distillation token\n",
    "# Copy class token weights if needed\n",
    "if 'cls_token' in student_state_dict:\n",
    "    inference_state_dict['cls_token'] = student_state_dict['cls_token']\n",
    "\n",
    "# 4. Load adjusted weights\n",
    "inference_model.load_state_dict(inference_state_dict, strict=False)\n",
    "inference_model = inference_model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57904a13",
   "metadata": {},
   "source": [
    "## Evaluation and Making Kaggle Submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92f376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(model, metadata_path, hdf5_path, output_file='submission.csv'):\n",
    "    # Load test metadata\n",
    "    test_meta = pd.read_csv(metadata_path)\n",
    "    \n",
    "    # Corrected transforms with proper tensor handling\n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(256, 256),\n",
    "        A.CenterCrop(224, 224),\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "            max_pixel_value=255.0\n",
    "        ),\n",
    "        ToTensorV2()  # Handles conversion to tensor and CHW format\n",
    "    ])\n",
    "\n",
    "    class ISICTestDataset(Dataset):\n",
    "        def __init__(self, df, h5_file_path, transform=None):\n",
    "            self.df = df\n",
    "            self.h5_file_path = h5_file_path\n",
    "            self.isic_ids = df['isic_id'].values\n",
    "            self.transform = transform\n",
    "            self.h5_file = None\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.isic_ids)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            if self.h5_file is None:\n",
    "                self.h5_file = h5py.File(self.h5_file_path, 'r')\n",
    "            \n",
    "            isic_id = self.isic_ids[idx]\n",
    "            image_data = np.array(Image.open(BytesIO(self.h5_file[isic_id][()])))\n",
    "            \n",
    "            if self.transform:\n",
    "                transformed = self.transform(image=image_data)\n",
    "                image = transformed[\"image\"]  # Already a tensor from ToTensorV2()\n",
    "            else:\n",
    "                # If no transform, convert to tensor here\n",
    "                image = torch.from_numpy(image_data).permute(2, 0, 1).float() / 255.0\n",
    "                \n",
    "            return {'image': image, 'isic_id': isic_id}\n",
    "\n",
    "    test_dataset = ISICTestDataset(test_meta, h5_file_path=hdf5_path, transform=val_transform)\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "    model.eval()\n",
    "    all_isic_ids = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Generating Predictions\"):\n",
    "            images = batch['image'].to(device)\n",
    "            outputs = model(images)\n",
    "            probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
    "            all_isic_ids.extend(batch['isic_id'])\n",
    "            all_probs.extend(probs)\n",
    "\n",
    "    submission_df = pd.DataFrame({\n",
    "        'isic_id': all_isic_ids,\n",
    "        'malignancy': all_probs\n",
    "    }).sort_values('isic_id')\n",
    "\n",
    "    submission_df.to_csv(output_file, index=False)\n",
    "    print(f\"Submission file created: {output_file}\")\n",
    "    submission_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22657353",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_submission(\n",
    "    inference_model,\n",
    "    metadata_path='./isic-2024-challenge/test-metadata.csv',\n",
    "    hdf5_path='./isic-2024-challenge/test-image.hdf5',\n",
    "    output_file='submission.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5978b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_transform = A.Compose([\n",
    "#     A.Resize(256, 256),\n",
    "#     A.Normalize(\n",
    "#         mean=[0.4815, 0.4578, 0.4082], \n",
    "#         std=[0.2686, 0.2613, 0.2758], \n",
    "#         max_pixel_value=255.0,\n",
    "#         p=1.0\n",
    "#     ),\n",
    "#     ToTensorV2(),\n",
    "# ])\n",
    "\n",
    "# # Prepare test dataset (no labels)\n",
    "# class ISICTestDataset(Dataset):\n",
    "#     def __init__(self, df, h5_file_path, transform=None):\n",
    "#         self.df = df\n",
    "#         self.h5_file_path = h5_file_path\n",
    "#         self.isic_ids = df['isic_id'].values\n",
    "#         self.transform = transform\n",
    "#         self.h5_file = None\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.isic_ids)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         if self.h5_file is None:\n",
    "#             self.h5_file = h5py.File(self.h5_file_path, mode=\"r\")\n",
    "#         isic_id = self.isic_ids[idx]\n",
    "#         img = np.array(Image.open(BytesIO(self.h5_file[isic_id][()])))\n",
    "#         if self.transform:\n",
    "#             img = self.transform(image=img)[\"image\"]\n",
    "#         return {'image': img, 'isic_id': isic_id}\n",
    "\n",
    "# # Load test metadata\n",
    "# test_metadata = pd.read_csv('./isic-2024-challenge/test-metadata.csv')\n",
    "\n",
    "# # Use the same transforms as validation\n",
    "# test_dataset = ISICTestDataset(\n",
    "#     test_metadata,\n",
    "#     h5_file_path='./isic-2024-challenge/test-image.hdf5',\n",
    "#     transform=test_transform\n",
    "# )\n",
    "\n",
    "# test_loader = DataLoader(\n",
    "#     test_dataset,\n",
    "#     batch_size=32,\n",
    "#     shuffle=False,\n",
    "#     num_workers=0\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9174eed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inference: 100%|██████████| 1/1 [00:00<00:00,  8.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved as submission.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isic_id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ISIC_0015657</td>\n",
       "      <td>0.331845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ISIC_0015729</td>\n",
       "      <td>0.295968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ISIC_0015740</td>\n",
       "      <td>0.329070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        isic_id    target\n",
       "0  ISIC_0015657  0.331845\n",
       "1  ISIC_0015729  0.295968\n",
       "2  ISIC_0015740  0.329070"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Instantiate the student model with the same config as training\n",
    "# student_model = DistillableViT(image_size=256, patch_size=16, num_classes=2)\n",
    "# student_model.load_state_dict(torch.load('best_student_model.pth'))\n",
    "# student_model.eval()\n",
    "\n",
    "# all_isic_ids = []\n",
    "# all_probs = []\n",
    "\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     for batch in tqdm(test_loader, desc=\"Inference\"):\n",
    "#         images = batch['image']\n",
    "#         isic_ids = batch['isic_id']\n",
    "#         outputs, _= student_model(images)\n",
    "#         probs = torch.sigmoid(outputs).cpu().numpy()\n",
    "#         all_isic_ids.extend(isic_ids)\n",
    "#         all_probs.extend(probs)\n",
    "\n",
    "# submission = pd.DataFrame({\n",
    "#     'isic_id': all_isic_ids,\n",
    "#     'target': [prob[1] for prob in all_probs]  # Extract second probability value\n",
    "# })\n",
    "\n",
    "# submission.to_csv('submission.csv', index=False)\n",
    "# print(\"Submission file saved as submission.csv\")\n",
    "# submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "545",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
